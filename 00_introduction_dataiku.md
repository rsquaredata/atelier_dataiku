# Module 0 ‚Äî Introduction √† Dataiku

## 1. Introduction et objectifs

Ce module introductif a pour but de replacer l'atelier Dataiku dans le contexte plus large de la data science appliqu√©e √† la finance.  
Il pr√©sente l'environnement de travail, les fondements de la plateforme Dataiku, et la d√©mo de mise en route qui permettra d'aborder les modules techniques suivants (scoring et d√©tection de fraude).

**Objectifs p√©dagogiques**

- Comprendre la raison d'√™tre et la philosophie de Dataiku.  
- Identifier les composants de l'interface et leur articulation.  
- Distinguer les diff√©rentes versions du produit et leurs usages.  
- R√©aliser pas √† pas la d√©mo de cr√©ation de projet et d'import de donn√©es.  
- Acqu√©rir une vue d'ensemble du cycle de vie d'un projet de data science dans Dataiku.  

---

## 2. Contexte et positionnement de Dataiku

### 2.1. Origine et d√©veloppement

**Dataiku** est une soci√©t√© fran√ßaise fond√©e √† Paris en 2013 par **Florian Douetteau**, **Thomas Cabrol**, **Cl√©ment Stenac** et **Marc Batty**.  
Leur ambition initiale : rendre la data science collaborative et accessible, sans d√©pendre exclusivement du code.  

Le produit phare, **Dataiku DSS (Data Science Studio)**, est devenu une plateforme de r√©f√©rence pour la cr√©ation, le d√©ploiement et la gouvernance de projets analytiques et de machine learning √† l'√©chelle d'une organisation.

### 2.2. Philosophie : ‚ÄúDemocratize Data Science‚Äù

L'objectif de Dataiku n'est pas de remplacer les data scientists, mais de cr√©er un **environnement partag√©** o√π diff√©rents profils (data engineers, analystes, m√©tiers) peuvent travailler ensemble :

- Les utilisateurs non techniques exploitent des **recipes visuelles**.  
- Les utilisateurs avanc√©s peuvent int√©grer du **code Python, R, SQL** au sein du m√™me flux.  
- L'outil favorise la **gouvernance** et la **tra√ßabilit√©** de chaque √©tape.

Cette approche est qualifi√©e de **tech-agnostique** : Dataiku ne privil√©gie pas une technologie ou un langage particulier, mais agit comme une couche d'orchestration entre eux.

---

## 3. √âcosyst√®me du produit

### 3.1. Versions principales

| Version                                | Description                                                      | Public cible                                |
| -------------------------------------- | ---------------------------------------------------------------- | ------------------------------------------- |
| **Dataiku DSS (Desktop)**              | Version install√©e localement, gratuite pour un usage individuel. | √âtudiants, ind√©pendants.                    |
| **Dataiku Cloud Trial**                | Version cloud h√©berg√©e (essai gratuit 14 jours).                 | Formation, d√©monstration.                   |
| **Dataiku Cloud Enterprise / Managed** | Version compl√®te pour entreprises (SaaS ou on-premise).          | Organisations, √©quipes pluridisciplinaires. |

### 3.2. Architecture logique

Un projet Dataiku est organis√© autour de quatre concepts fondamentaux :

| Composant    | R√¥le                                                    |
| ------------ | ------------------------------------------------------- |
| **Dataset**  | Source de donn√©es (fichier, base SQL, API, connecteur). |
| **Recipe**   | Op√©ration de transformation ou de mod√©lisation.         |
| **Flow**     | Vue d'ensemble du pipeline de traitement.               |
| **Scenario** | Automatisation ou planification d'ex√©cutions.           |

Des modules compl√©mentaires assurent l'int√©gration :  
- **Dashboards** (visualisation et restitution)  
- **LLM Recipes / Agents** (int√©gration de mod√®les de langage et IA g√©n√©rative)  
- **Project Library / Plugins** (extensibilit√© de l'environnement)

### 3.3. Connecteurs et interop√©rabilit√©

Dataiku dispose de connecteurs vers :  
- Bases de donn√©es : PostgreSQL, MySQL, Oracle, Snowflake, BigQuery.  
- Cloud storages : AWS S3, Azure Blob, Google Drive.  
- Services d'analyse : Tableau, Power BI, Qlik.  
- Environnements de d√©veloppement : compatibilit√© directe avec **Python (venv ou conda)**, **R**, **Spark**, **SQL**.

Les biblioth√®ques Python ou R n√©cessaires sont g√©r√©es par **code environnements** internes : chaque projet peut poss√©der un environnement virtuel isol√©.

---

## 4. D√©marche g√©n√©rale d'un projet Dataiku

1. **Ingestion des donn√©es**  
   (Upload, connecteur SQL, API).  
2. **Pr√©paration et nettoyage**  
   (recipes visuelles ou code).  
3. **Analyse exploratoire**  
   (visualisations, statistiques descriptives).  
4. **Mod√©lisation**  
   (AutoML, Python, R, deep learning).  
5. **D√©ploiement et automatisation**  
   (Scenarios, API Endpoints, Agents).  
6. **Gouvernance et monitoring**  
   (drift detection, audit, permissions).

---

### Comparatif ‚Äî Dataiku vs autres outils de l'√©cosyst√®me data

| Outil | Type principal | Approche | Niveau de transparence | Points forts | Limites |
|--------|----------------|-----------|-------------------------|---------------|----------|
| **Dataiku** | Plateforme de **data science** et **ML** | **Whitebox** (no-code & code-friendly) | Tr√®s √©lev√© : tra√ßabilit√© du Flow, explicabilit√© des mod√®les, logs, gouvernance | Collaboration, tra√ßabilit√©, int√©gration Python/R, gouvernance | N√©cessite un minimum de structure de projet |
| **Power BI** | Outil de **Business Intelligence (BI)** | **Semi-whitebox** (formules DAX visibles mais moteur partiellement opaque) | Moyenne : scripts Power Query transparents, moteur interne ferm√© | Visualisations interactives, int√©gration Microsoft, facilit√© d'usage | Explicabilit√© faible, logique de calcul ferm√©e |
| **Qlik Sense / Qlik View** | **BI associatif** | **Semi-whitebox** (scripts ETL visibles, moteur propri√©taire opaque) | ‚öôÔ∏è Moyenne : logique de chargement lisible, algorithme associatif non document√© | Analyse associative rapide, exploration intuitive | Opaque sur le moteur interne et calculs m√©moire |
| **Apache Hop** | Outil **ETL open source** | **Whitebox (open source)** | √âlev√©e : workflows et scripts enti√®rement visibles | Transparence, flexibilit√©, extensibilit√© | Pas d'interface analytique ni AutoML |
| **Apache Doris** | **Base analytique distribu√©e (OLAP)** | **Whitebox (open source)** | √âlev√©e c√¥t√© code source, mais faible c√¥t√© interface utilisateur | Performances massives, SQL analytique, open source | R√©serv√© aux profils techniques, pas d'interface visuelle |
| **Apache NiFi** | Outil d'**ingestion et d'orchestration de flux** | **Whitebox (open source)** | √âlev√©e : dataflows visibles, provenance et tra√ßabilit√© natives | Ingestion en temps r√©el, connecteurs multiples, int√©gration avec Dataiku via API | Pas de moteur analytique int√©gr√©, courbe d'apprentissage initiale |

---

## 3. D√©monstration ‚Äî Import de taux de change (Banque centrale europ√©enne)

### Jeu de donn√©es

- **Source :** Banque centrale europ√©enne (ECB)  
- **Acc√®s public :** [https://nbs.sk/export/en/exchange-rate/latest/csv](https://nbs.sk/export/en/exchange-rate/latest/csv)  
- **Format :** CSV (actualis√© chaque jour ouvr√© vers 16:00 CET).  
- **Colonnes :** `Date`, `Currency`, `Country`, `Amount`, `Rate`.

---

### 3.2. √âtapes de la d√©monstration

#### √âtape 1 : cr√©ation du projet

1. Se connecter sur <https://profile.dataiku.com/> puis **Start free trial ‚Üí Dataiku Cloud**.  
2. Cr√©er un projet nomm√© :  
   - **Name :** `Dataiku_Intro_[Pr√©nomNom]`  
   - **Key :** `Intro_[initiales]`  
   - Cliquer sur **Create Project**.

#### √âtape 2 : cr√©ation d'un dossier HTTP

1. Dans la vue **Flow**, cliquer sur **+ New ‚Üí Folder ‚Üí HTTP**.  
2. Dans le champ **URL**, coller :  
   ```
   https://nbs.sk/export/en/exchange-rate/latest/csv
   ```  
3. Cliquer sur **Test & List Files** ‚Üí v√©rifier qu'un fichier appara√Æt.  
4. Valider ‚Üí **Create**.  
5. Renommer le dossier : `ecb_rates_folder`.

#### √âtape 3 : cr√©ation du dataset

1. Dans le **Flow**, cliquer sur **+ Dataset ‚Üí Files in Folder**.  
2. S√©lectionner le dossier `ecb_rates_folder`.  
3. Laisser le fichier d√©tect√© par d√©faut.  
4. Nommer le dataset : **`fx_rates`**.  
5. Cliquer sur **Create** puis ouvrir le dataset.  

#### √âtape 4 : exploration

1. Onglet **Explore** : visualiser les colonnes (`Currency`, `Rate`, `Date`, etc.).  
2. Trier par `Rate` pour identifier les devises les plus fortes/faibles.  
3. Cliquer sur **Statistics ‚Üí Descriptive Statistics** : observer la distribution des taux.

**Question :**  
- Quelle devise a actuellement le taux de conversion le plus √©lev√© ?  
  <details><summary>üí°</summary>Les devises peu courantes comme l'ISK (couronne islandaise) ou le HUF (forint hongrois) affichent souvent les taux les plus √©lev√©s.</details>

---

#### √âtape 5 : pr√©paration des donn√©es

1. Depuis le **Flow**, s√©lectionner `fx_rates` ‚Üí **+ Recipe ‚Üí Prepare**.  
2. Nommer la sortie : **`fx_rates_cleaned`**.  
3. Dans l'√©diteur Prepare :  
   - **Supprimer** les lignes vides s'il y en a.  
   - **Renommer** les colonnes si n√©cessaire (par ex. `Rate` ‚Üí `EUR_per_unit`).  
   - **Cr√©er** une nouvelle colonne :  
     ```
     Inverse_rate = 1 / EUR_per_unit
     ```  
     (taux inverse, utile pour exprimer la valeur d'un EUR en devise locale).  
4. Ex√©cuter la Recipe.  
5. Ouvrir `fx_rates_cleaned` et v√©rifier les nouvelles colonnes.

**Question :**  
- Pourquoi peut-il √™tre utile de calculer le taux inverse ?  
  <details><summary>R√©ponse</summary>Parce qu'il permet d'exprimer le montant d'euros obtenu pour une unit√© de devise √©trang√®re, ce qui facilite la comparaison dans les deux sens.</details>

---

#### √âtape 6 : visualisation et tableau de bord

1. Dans le Flow, s√©lectionner `fx_rates_cleaned` ‚Üí **+ New ‚Üí Dashboard**.  
2. Nommer le tableau de bord : **`fx_dashboard`**.  
3. Ajouter deux graphiques :  
   - **Bar chart :** `Currency` en x, `EUR_per_unit` en y.  
   - **Table :** liste compl√®te des devises et des taux.  
4. Sauvegarder.

**Flow attendu :**  
```
fx_rates ‚Üí fx_rates_cleaned ‚Üí fx_dashboard
```

---

## 4. V√©rification du Flow

Chaque n≈ìud doit √™tre connect√© et nomm√© ainsi :
| Type           | Nom                         | Description                                 |
| -------------- | --------------------------- | ------------------------------------------- |
| Dossier HTTP   | `ecb_rates_folder`          | Contient le fichier CSV distant.            |
| Dataset        | `fx_rates`                  | Donn√©es brutes import√©es depuis le dossier. |
| Recipe Prepare | (output) `fx_rates_cleaned` | Donn√©es nettoy√©es et enrichies.             |
| Dashboard      | `fx_dashboard`              | Visualisation synth√©tique.                  |

---

## 5. Transparence et approche ¬´ whitebox ¬ª

L'un des int√©r√™ts majeurs de Dataiku r√©side dans son approche **whitebox** :  
plut√¥t que de masquer la logique interne des traitements, la plateforme rend chaque op√©ration **lisible, tra√ßable et reproductible**.

- Les **recipes** sont document√©es et visualisables dans le **Flow**, garantissant la transparence du pipeline.  
- Les **mod√®les de machine learning** exposent leurs **coefficients, m√©triques et graphiques d'explicabilit√©**.  
- Les **scenarios** et **logs d'ex√©cution** permettent de suivre pr√©cis√©ment les actions r√©alis√©es.  

Cette philosophie s'oppose aux outils dits ‚Äúblackbox‚Äù qui produisent un r√©sultat sans permettre de comprendre le cheminement.  
Elle est particuli√®rement cruciale en **finance**, o√π la **tra√ßabilit√©, la justification et la gouvernance des mod√®les** sont des obligations r√©glementaires.

---

## 6. Perspectives m√©tier

- Les taux de change publi√©s par la Banque centrale europ√©enne constituent une **r√©f√©rence officielle** pour de nombreuses institutions financi√®res.  
- Cette d√©monstration illustre comment un analyste peut :  
  - importer automatiquement des donn√©es ouvertes ;  
  - effectuer des transformations simples ;  
  - visualiser les r√©sultats dans un tableau de bord interactif.  
- Aucune ligne de code n'est n√©cessaire pour produire une analyse quotidienne reproductible.

On note que l'usage de Dataiku en finance permet de concilier exigences de rigueur analytique et besoins op√©rationnels :

- **Productivit√©** : automatisation de t√¢ches r√©p√©titives (pr√©paration, scoring, reporting).  
- **Tra√ßabilit√©** : chaque transformation est enregistr√©e, facilitant les audits r√©glementaires (B√¢le III, ESG).  
- **Interop√©rabilit√©** : int√©gration simple avec les syst√®mes d'information bancaires existants.  
- **Collaboration** : les √©quipes data, IT et m√©tier travaillent sur une plateforme commune.

L'objectif n'est pas de remplacer les sp√©cialistes techniques, mais de renforcer la gouvernance et la reproductibilit√© des projets.

---

## 8. Ressources

- Pr√©sentation Dataiku : <https://www.dataiku.com/product/>  
- Documentation Dataiku : <https://doc.dataiku.com/>  
- Dataiku Academy (cours en ligne gratuits) : <https://academy.dataiku.com/>  
