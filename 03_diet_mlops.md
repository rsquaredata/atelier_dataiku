# Module 3 - Automatisation, Agents et LLM (Diet MLOps)

Ce module cl√¥t l'atelier en abordant la mise en production simplifi√©e des projets Dataiku ‚Äî une approche que nous appellerons ici, par commodit√©, ¬´ Diet MLOps ¬ª (c'est plus sympa que Light ou Zero).
L'id√©e est d'en pr√©senter les principes essentiels sans entrer dans la complexit√© d'une infrastructure compl√®te de production.

L'objectif du module est de montrer comment :
- automatiser le r√©entra√Ænement et le scoring d'un mod√®le ;  
- int√©grer un LLM pour enrichir l'interpr√©tation m√©tier ;  
- superviser le pipeline √† l'aide de sc√©narios et de dashboards.

Cette approche illustre comment Dataiku permet de relier, dans un m√™me environnement, la rigueur technique du MLOps et la lisibilit√© m√©tier attendue dans un contexte professionnel.

---

<details>
  <summary><strong>üí°</strong></summary>

## Rappels th√©oriques - concepts cl√©s

### Qu'est-ce que le MLOps ?
Le MLOps (Machine Learning Operations) d√©signe l'ensemble des pratiques qui permettent de d√©ployer, surveiller et maintenir des mod√®les de machine learning en production. Son objectif est d'industrialiser la cha√Æne de valeur de la data science: passer du prototype √† un syst√®me exploitable, fiable et maintenable.

Un pipeline MLOps typique comprend:
1. Ingestion des donn√©es (collecte et validation)
2. Pr√©paration et feature engineering
3. Entra√Ænement du mod√®le
4. D√©ploiement en production
5. Supervision et alertes
6. It√©ration et r√©entra√Ænement

B√©n√©fices principaux: automatisation, tra√ßabilit√©, reproductibilit√© et gouvernance du mod√®le. Dataiku int√®gre ces fonctions via les **Scenarios**, le **Model Versioning** et les **Metrics Stores**.

---

### Automatisation: du pipeline au sc√©nario
Un sc√©nario Dataiku est une suite d'actions ex√©cut√©es automatiquement selon un d√©clencheur (horaire, changement de dataset, ex√©cution manuelle, etc.). Il permet par exemple de:
- actualiser les donn√©es brutes;
- r√©entra√Æner un mod√®le existant;
- lancer des pr√©dictions;
- notifier une √©quipe (e-mail, webhook, Slack, etc.).

L'automatisation r√©duit le risque d'erreur humaine et garantit la coh√©rence du pipeline au fil du temps.

---

### Agents intelligents et orchestration
Les Agents sont des entit√©s logicielles capables d'ex√©cuter des t√¢ches ou de r√©pondre √† des requ√™tes en utilisant des donn√©es et des mod√®les internes.
Dans Dataiku, ils peuvent:
- appeler des mod√®les pr√©dictifs existants (par ex. `fraud_xgboost_model`);
- utiliser une **LLM Recipe** pour interagir en langage naturel;
- produire un rapport ou exporter des cas √† v√©rifier.

Ils combinent plusieurs briques: automatisation, acc√®s contextuel aux donn√©es, et interface conversationnelle.

---

### LLM: mod√®les de langage et int√©gration API
Les LLM (Large Language Models) sont des mod√®les pr√©-entra√Æn√©s sur de grandes quantit√©s de texte, capables de g√©n√©rer, r√©sumer ou reformuler du langage naturel. Dataiku permet d'int√©grer ces mod√®les via une **LLM Recipe**, en connectant une API externe comme Mistral, OpenAI ou Claude.

Principe:
1. Cr√©er un connecteur d'API s√©curis√© (cl√© priv√©e).
2. R√©diger une prompt adapt√©e au contexte m√©tier.
3. Ex√©cuter la recette pour g√©n√©rer un texte ou une d√©cision.

Exemple: faire r√©sumer par le LLM les transactions suspectes pour produire un rapport automatique.

---

### Supervision et surveillance des mod√®les
Une fois en production, les mod√®les doivent √™tre surveill√©s pour d√©tecter les d√©rives:
- Data drift: changement dans la distribution des variables.
- Concept drift: changement de la relation entre les variables et la cible.
- Performance drift: baisse du Recall, de la Precision ou du F1-score.

Dataiku propose un Model Evaluation Store pour centraliser et comparer les m√©triques au fil du temps.

---

### Glossaire
| Terme | D√©finition concise |
|-------|--------------------|
| MLOps | Pratiques visant √† automatiser et fiabiliser le cycle de vie des mod√®les ML |
| Pipeline | Encha√Ænement de t√¢ches: pr√©paration -> mod√©lisation -> pr√©diction -> surveillance |
| Sc√©nario | Suite d'actions ex√©cut√©es automatiquement dans Dataiku |
| Agent | Entit√© logicielle autonome capable d'agir ou de r√©pondre √† une demande |
| LLM | Mod√®le de langage massif, utilis√© pour g√©n√©rer ou interpr√©ter du texte |
| Drift | Evolution des donn√©es ou du comportement du mod√®le dans le temps |
| Monitoring | Suivi continu des performances et alertes automatiques |

---

</details>

## 2. TP - Automatisation et int√©gration Mistral API

### A. Cr√©ation d'un sc√©nario d'automatisation
Objectif: cr√©er un pipeline automatique de mise √† jour et d'√©valuation du mod√®le de d√©tection de fraude.

1. Dans le projet, ouvrir **Scenarios -> + New Scenario**.
   - Nom: `auto_fraud_pipeline`
   - Trigger: **Manually** (pour commencer)
2. Ajouter une √©tape: **+ Step -> Build / Dataset(s)**
   - S√©lectionner les datasets: `fraud_hour`, `fraud_sampled`, `fraud_prediction`
3. Ajouter une √©tape: **+ Step -> Train model(s)**
   - S√©lectionner le mod√®le `fraud_xgboost_model`
4. Ajouter une √©tape: **+ Step -> Run Scoring recipe(s)**
   - Choisir la recette `fraud_prediction`
5. Optionnel: **+ Step -> Send message**
   - Destinataire : votre e-mail
   - Message  : "Pipeline termin√© : mod√®le fraud r√©entra√Æn√© et scor√©."
6. Enregistrer le sc√©nario puis ex√©cuter : **Run Now**

### Questions
a. Quels avantages apporte l'automatisation √† ce stade du projet ?  
b. Pourquoi est-il important d'int√©grer la phase de r√©entra√Ænement dans le sc√©nario ?

<details>
  <summary><strong>üí°</strong></summary>

a. L'automatisation r√©duit les erreurs manuelles, garantit la reproductibilit√© et lib√®re du temps pour l'analyse.  
b. Le r√©entra√Ænement p√©riodique permet de s'adapter aux √©volutions des donn√©es et de pr√©venir la d√©rive du mod√®le.

</details>

---

### B. Cr√©ation d'un Agent avec LLM Recipe (Mistral API)
Objectif : cr√©er un agent capable de produire automatiquement une synth√®se textuelle des transactions suspectes √† partir des pr√©dictions.

1. Dans le **Flow**, s√©lectionner `fraud_prediction` -> **+ Recipe -> LLM -> Create Recipe**
   - Nom : `risk_explanation`
2. Choisir **Mistral API** comme fournisseur LLM
   - Si l'API n'est pas configur√©e :
     - **Settings -> Integrations -> LLMs -> + Add LLM Provider**
     - Fournisseur : Mistral
     - Cl√© API : coller la cl√© fournie (`sk-...`)
3. Dans la LLM Recipe :
   - Prompt :
     ```
     Vous √™tes un analyste conformit√©. R√©sumez en quelques lignes pourquoi cette transaction est consid√©r√©e comme potentiellement frauduleuse, en vous appuyant sur les variables les plus importantes du mod√®le.
     ```
   - Entr√©e : `fraud_prediction`
   - Sortie : `risk_explanation`
   - Ex√©cuter : **Run**
4. Cr√©er un **Agent** : **+ New -> Agent -> Blank Agent**
   - Nom : `RiskAnalystBot`
   - Action : Analyser les transactions √† risque
   - Comportement :
     - Entr√©e : `fraud_prediction`
     - Sortie : `cases_to_review.csv`
   - Ex√©cuter : **Run**

### Questions
a. Quel r√¥le joue le LLM dans ce pipeline ?  
b. En quoi l'utilisation d'un agent compl√®te l'approche MLOps ?  
c. Quels risques ou limites √©thiques cela introduit-il ?

<details>
  <summary><strong>üí°</strong></summary>

a. Le LLM produit une interpr√©tation en langage naturel des r√©sultats du mod√®le, facilitant la lecture pour un analyste non technique.  
b. L'agent automatise la d√©cision post-pr√©diction, ce qui √©tend la cha√Æne MLOps jusqu'√† la communication.  
c. Risques : hallucinations, biais, fuites de donn√©es sensibles. D'o√π l'importance du contr√¥le humain et des garde-fous pour la science et la gouvernance des donn√©es !

</details>

---

### C. Supervision, alertes et tableau de bord

1. Cr√©er un dashboard de supervision : **+ New -> Dashboard -> llm_performance**
   - Ajouter :
     - Historique des versions de mod√®les (Model Evaluation Store)
     - Performance du mod√®le (Precision, Recall, AUC-PR)
     - Indicateur de d√©rive des donn√©es
     - Liste des derni√®res explications g√©n√©r√©es par le LLM
2. Cr√©er un sc√©nario de contr√¥le : **+ New Scenario -> health_check_llm**
   - Etape 1 : V√©rifier les m√©triques du mod√®le (`fraud_xgboost_model`)
   - Etape 2 : Tester la disponibilit√© de l'API Mistral (ping)
   - Etape 3 : Envoi automatique d'un e-mail si l'une des v√©rifications √©choue

### Questions
a. Pourquoi surveiller √† la fois le mod√®le et l'API LLM ?  
b. Quels indicateurs peuvent signaler une d√©rive ?  
c. Quelle serait la r√©action appropri√©e en cas de d√©rive d√©tect√©e ?

<details>
  <summary><strong>üí°</strong></summary>

a. Le mod√®le et le LLM sont deux points de d√©faillance : si l'un faiblit, la cha√Æne compl√®te est compromise.  
b. Baisse du Recall, hausse du taux d'erreur, variation des distributions ou latence excessive de l'API.  
c. Examiner les logs, r√©entra√Æner le mod√®le si n√©cessaire, ou ajuster les seuils et prompts.

---

## 3. Perspective m√©tier - mise en production responsable dans Dataiku
La mise en production d'un pipeline Dataiku int√©grant des mod√®les et des LLM engage la responsabilit√© op√©rationnelle et la qualit√© du pilotage.

- Tra√ßabilit√© et documentation :
  chaque composant du Flow (dataset, recipe, mod√®le, sc√©nario) est historis√©. Le Model Evaluation Store conserve les versions et leurs m√©triques, permettant d'auditer les choix.
- Validation humaine et gouvernance :
  Dataiku ne remplace pas le contr√¥le m√©tier. Les recettes automatis√©es et les agents doivent √™tre supervis√©s par un analyste ou un data scientist avant toute d√©cision op√©rationnelle. L'export `cases_to_review.csv` mat√©rialise ce principe.
- Co√ªt et scalabilit√© :
  l'int√©gration d'un LLM via API (ici Mistral) a un co√ªt proportionnel aux requ√™tes. Calibrer la fr√©quence et cibler les usages √† forte valeur ajout√©e. Les sc√©narios peuvent conditionner l'ex√©cution de la LLM Recipe aux seuls cas suspects.
- Surveillance continue et it√©ration :
  le sc√©nario `health_check_llm` illustre la capacit√© de Dataiku √† d√©clencher des alertes ou des r√©entra√Ænements automatiques selon des m√©triques d√©finies.

---

## Ressources
- Documentation Dataiku - Scenarios : https ://doc.dataiku.com/dss/latest/scenarios/index.html
- Dataiku - Agents and LLM Integrations : https ://doc.dataiku.com/dss/latest/agents/index.html
- Mistral API Reference : https://docs.mistral.ai
- Dataiku Academy - MLOps Concepts: https://academy.dataiku.com

</details>