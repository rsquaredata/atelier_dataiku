# Module 3 - Automatisation, Agents et LLM (Diet MLOps)

Ce module aborde la mise en production simplifi√©e des projets Dataiku ‚Äî une approche que nous appellerons ici, par commodit√©, ¬´ *Diet MLOps* ¬ª (ce qui est plus engageant que *Light* ou *Zero*).  
L'id√©e est d'en pr√©senter les principes essentiels sans entrer dans la complexit√© d'une infrastructure compl√®te de production.

L'objectif du module est de montrer comment :
- automatiser le r√©entra√Ænement et le scoring d'un mod√®le ;  
- int√©grer un LLM pour enrichir l'interpr√©tation m√©tier ;  
- superviser le pipeline √† l'aide de sc√©narios et de dashboards.

Cette approche illustre comment Dataiku permet de relier, dans un m√™me environnement, la rigueur technique du MLOps et la lisibilit√© m√©tier attendue dans un contexte professionnel.

---

<details>
  <summary><strong></strong></summary>

## Rappels th√©oriques - concepts cl√©s

### Qu'est-ce que le MLOps ?
Le MLOps (Machine Learning Operations) d√©signe l'ensemble des pratiques qui permettent de d√©ployer, surveiller et maintenir des mod√®les de machine learning en production. Son objectif est d'industrialiser la cha√Æne de valeur de la data science: passer du prototype √† un syst√®me exploitable, fiable et maintenable.

Un pipeline MLOps typique comprend:
1. Ingestion des donn√©es (collecte et validation)
2. Pr√©paration et feature engineering
3. Entra√Ænement du mod√®le
4. D√©ploiement en production
5. Supervision et alertes
6. It√©ration et r√©entra√Ænement

B√©n√©fices principaux : automatisation, tra√ßabilit√©, reproductibilit√© et gouvernance du mod√®le. Dataiku int√®gre ces fonctions via les **Scenarios**, le **Model Versioning** et les **Metrics Stores**.

---

### Automatisation : du pipeline au sc√©nario
Un sc√©nario Dataiku est une suite d'actions ex√©cut√©es automatiquement selon un d√©clencheur (horaire, changement de dataset, ex√©cution manuelle, etc.). Il permet par exemple de :
- actualiser les donn√©es brutes;
- r√©entra√Æner un mod√®le existant;
- lancer des pr√©dictions;
- notifier une √©quipe (e-mail, webhook, Slack, etc.).

L'automatisation r√©duit le risque d'erreur humaine et garantit la coh√©rence du pipeline au fil du temps.

---

### Agents intelligents et orchestration
Les Agents sont des entit√©s logicielles capables d'ex√©cuter des t√¢ches ou de r√©pondre √† des requ√™tes en utilisant des donn√©es et des mod√®les internes.
Dans Dataiku, ils peuvent :
- appeler des mod√®les pr√©dictifs existants (par ex. `fraud_xgboost_model`);
- utiliser une **LLM Recipe** pour interagir en langage naturel;
- produire un rapport ou exporter des cas √† v√©rifier.

Ils combinent plusieurs briques : automatisation, acc√®s contextuel aux donn√©es, et interface conversationnelle.

---

### LLM: mod√®les de langage et int√©gration API
Les LLM (Large Language Models) sont des mod√®les pr√©-entra√Æn√©s sur de grandes quantit√©s de texte, capables de g√©n√©rer, r√©sumer ou reformuler du langage naturel. Dataiku permet d'int√©grer ces mod√®les via une **LLM Recipe**, en connectant une API externe comme Mistral, OpenAI ou Claude.

Principe :
1. Cr√©er un connecteur d'API s√©curis√© (cl√© priv√©e).
2. R√©diger une prompt adapt√©e au contexte m√©tier.
3. Ex√©cuter la recette pour g√©n√©rer un texte ou une d√©cision.

Exemple : faire r√©sumer par le LLM les transactions suspectes pour produire un rapport automatique.

---

### Supervision et surveillance des mod√®les
Une fois en production, les mod√®les doivent √™tre surveill√©s pour d√©tecter les d√©rives:
- Data drift : changement dans la distribution des variables.
- Concept drift : changement de la relation entre les variables et la cible.
- Performance drift : baisse du Recall, de la Precision ou du F1-score.

Dataiku propose un Model Evaluation Store pour centraliser et comparer les m√©triques au fil du temps.

---

### Glossaire
| Terme | D√©finition concise |
|-------|--------------------|
| MLOps | Pratiques visant √† automatiser et fiabiliser le cycle de vie des mod√®les ML |
| Pipeline | Encha√Ænement de t√¢ches : pr√©paration -> mod√©lisation -> pr√©diction -> surveillance |
| Sc√©nario | Suite d'actions ex√©cut√©es automatiquement dans Dataiku |
| Agent | Entit√© logicielle autonome capable d'agir ou de r√©pondre √† une demande |
| LLM | Mod√®le de langage massif, utilis√© pour g√©n√©rer ou interpr√©ter du texte |
| Drift | Evolution des donn√©es ou du comportement du mod√®le dans le temps |
| Monitoring | Suivi continu des performances et alertes automatiques |

---

</details>

## TP - Automatisation, Agents, LLM

### A. Cr√©ation d'un sc√©nario d'automatisation
Objectif : cr√©er un pipeline automatique de mise √† jour et d'√©valuation du mod√®le de d√©tection de fraude.

Au pr√©alable se rendre dans **Administration -> Settings -> Notifications & Integrations -> Messaging channels** et remplir les donn√©es comme ci-dessous :
![SMTP](/imgs/smtp.png)

Mot de passe : mmcf nkmo gkdy udtq

1. Dans le projet, ouvrir **Scenarios -> + New Scenario**.
   - Nom : `auto_fraud_pipeline`
   - Trigger : **Manually** (pour commencer)
2. Ajouter une √©tape : **+ Step -> Build / Dataset(s)**
   - S√©lectionner les datasets : `fraud_hour`, `fraud_sampled`, `fraud_prediction`
3. Ajouter une √©tape : **+ Step -> Train model(s)**
   - S√©lectionner le mod√®le `fraud_xgboost_model`
4. **+ Step -> Send message**
   - Destinataire : votre e-mail
   - Message  : "Pipeline termin√© : mod√®le fraud r√©entra√Æn√© et scor√©."
   - Ajouter vos pi√®ces dans **Attachments** (par exemple : votre analyse du mod√®le)
5. Enregistrer le sc√©nario puis ex√©cuter : **Run**

### Questions
a. Quels avantages apporte l'automatisation √† ce stade du projet ?  
b. Pourquoi est-il important d'int√©grer la phase de r√©entra√Ænement dans le sc√©nario ?

<details>
  <summary><strong>üí°</strong></summary>

a. L'automatisation r√©duit les erreurs manuelles, garantit la reproductibilit√© et lib√®re du temps pour l'analyse.  
b. Le r√©entra√Ænement p√©riodique permet de s'adapter aux √©volutions des donn√©es et de pr√©venir la d√©rive du mod√®le.

</details>

> **MLOps - Bonne pratique** : Les **Scenarios** dans Dataiku peuvent √™tre configur√©s pour ne s‚Äôex√©cuter qu‚Äôen cas de succ√®s des √©tapes pr√©c√©dentes. Cela permet d‚Äô√©viter des r√©entra√Ænements en cascade en cas d‚Äôerreur et renforce la fiabilit√© du pipeline.

---

### B. Cr√©ation d'un Agent avec LLM Recipe
Objectif : cr√©er un agent capable de produire automatiquement une synth√®se textuelle des transactions suspectes √† partir des pr√©dictions.

1. Dans le **Flow**, s√©lectionner `fraud_prediction` -> **+ Recipe -> LLM -> Create Recipe**
   - Nom : `risk_explanation`
2. Choisir **Mistral API** comme fournisseur LLM
   - Si l'API n'est pas configur√©e :
     - **Administration -> Connections -> + New Connection -> Mistral AI**
     - Cl√© API : coller la cl√© fournie (`votre_cl√©_api`)
3. Dans la LLM Prompt :
   - Prompt :
     ```
     Vous √™tes un analyste conformit√©.
     R√©sumez en quelques lignes pourquoi cette transaction est consid√©r√©e comme potentiellement frauduleuse, en vous appuyant sur les variables les plus importantes du mod√®le.
     ```
   - Entr√©e : `fraud_prediction`
   - Ex√©cuter : **Run**
4. Tester un **Agent** : **+ New -> Agent Tools**
   - Nom : `AnalystBot`
   - Action : Tester en lui donnant des instructions du type "Retourne les lignes o√π is_fraud = 1"
   - Ex√©cuter : **Run Test**

### Questions
a. Quel r√¥le joue le LLM dans ce pipeline ?  
b. En quoi l'utilisation d'un agent compl√®te l'approche MLOps ?  
c. Quels risques ou limites √©thiques cela introduit-il ?

<details>
  <summary><strong>üí°</strong></summary>

a. Le LLM produit une interpr√©tation en langage naturel des r√©sultats du mod√®le, facilitant la lecture pour un analyste non technique.  
b. L'agent automatise la d√©cision post-pr√©diction, ce qui √©tend la cha√Æne MLOps jusqu'√† la communication.  
c. Risques : hallucinations, biais, fuites de donn√©es sensibles. D'o√π l'importance du contr√¥le humain et des garde-fous pour la science et la gouvernance des donn√©es !

</details>

> **D√©pendance aux APIs LLM** : Les **LLM Recipes** reposent sur des services externes (OpenAI, Mistral, etc.). Cela implique des contraintes de confidentialit√©, de co√ªt et de souverainet√© : une cl√© API expos√©e ou un quota d√©pass√© peut interrompre la cha√Æne. Toujours pr√©voir un *fallback* (sc√©nario de repli) en cas d'indisponibilit√© de l'API.

---

### C. Supervision, alertes et tableau de bord

1. Cr√©er un dashboard de supervision : **Dashboard -> llm_performance**
   - Ajouter :
     - Historique des versions de mod√®les (Model Evaluation Store)
     - Performance du mod√®le (Precision, Recall, AUC-PR)
     - Indicateur de d√©rive des donn√©es
     - Liste des derni√®res explications g√©n√©r√©es par le LLM
2. (Bonus Facile) Cr√©er un sc√©nario de contr√¥le : **Sc√©nario -> + New Scenario -> health_check_llm**
   Objectif : Exporter le dashboard cr√©√© dans le point pr√©c√©dent dans un e-mail

### Questions (Pour poussez plus loin)
a. Pourquoi pourrait-il √™tre int√©ressant de surveiller √† la fois le mod√®le et l'API LLM ?  
b. Quels indicateurs peuvent signaler une d√©rive ?  
c. Quelle serait la r√©action appropri√©e en cas de d√©rive d√©tect√©e ?

<details>
  <summary><strong>üí°</strong></summary>

a. Le mod√®le et le LLM sont deux points de d√©faillance : si l'un faiblit, la cha√Æne compl√®te est compromise.  
b. Baisse du Recall, hausse du taux d'erreur, variation des distributions ou latence excessive de l'API.  
c. Examiner les logs, r√©entra√Æner le mod√®le si n√©cessaire, ou ajuster les seuils et prompts.

> **Exemple - D√©tection d‚Äôune d√©rive** : une variation anormale du Recall accompagn√©e d‚Äôun d√©placement de la distribution des variables (`amount`, `hour`) peut r√©v√©ler une d√©rive des donn√©es. Dans Dataiku, ce suivi se met en place via le **Model Evaluation Store** coupl√© √† un **Scenario** de contr√¥le automatique.

---

## Perspective m√©tier - mise en production responsable dans Dataiku
La mise en production d'un pipeline Dataiku int√©grant des mod√®les et des LLM engage la responsabilit√© op√©rationnelle et la qualit√© du pilotage.

- Tra√ßabilit√© et documentation :
  chaque composant du Flow (dataset, recipe, mod√®le, sc√©nario) est historis√©. Le Model Evaluation Store conserve les versions et leurs m√©triques, permettant d'auditer les choix.
- Validation humaine et gouvernance :
  Dataiku ne remplace pas le contr√¥le m√©tier. Les recettes automatis√©es et les agents doivent √™tre supervis√©s par un analyste ou un data scientist avant toute d√©cision op√©rationnelle. L'export `cases_to_review.csv` mat√©rialise ce principe.
- Co√ªt et scalabilit√© :
  l'int√©gration d'un LLM via API (ici Mistral) a un co√ªt proportionnel aux requ√™tes. Calibrer la fr√©quence et cibler les usages √† forte valeur ajout√©e. Les sc√©narios peuvent conditionner l'ex√©cution de la LLM Recipe aux seuls cas suspects.
- Surveillance continue et it√©ration :
  le sc√©nario `health_check_llm` illustre la capacit√© de Dataiku √† d√©clencher des alertes ou des r√©entra√Ænements automatiques selon des m√©triques d√©finies.

---

## Ressources
- Documentation Dataiku - Scenarios : https://doc.dataiku.com/dss/latest/scenarios/index.html
- Dataiku - Agents and LLM Integrations : https://doc.dataiku.com/dss/latest/agents/index.html
- Mistral API Reference : https://docs.mistral.ai
- Dataiku Academy - MLOps Concepts : https://academy.dataiku.com

</details>

---

<small>[**Page d'accueil**](https://github.com/rsquaredata/atelier_dataiku/blob/main/README.md)</small>
